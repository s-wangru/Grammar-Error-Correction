{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3b1c2c-825f-4c5b-9621-237ec446307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e00fb29-e5fa-44d1-a721-2ded4b1d9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\z910l567\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5EncoderModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a6064f-9e1c-4db1-8553-b2cf72059b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 23 13:09:51 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000             WDDM  | 00000000:51:00.0 Off |                  Off |\n",
      "| 37%   56C    P8              24W / 300W |    548MiB / 49140MiB |     27%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3272    C+G   ...05.0_x64__8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A      4228    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      7464    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      8704    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     10004    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12476    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12924    C+G   ...on\\HEX\\Creative Cloud UI Helper.exe    N/A      |\n",
      "|    0   N/A  N/A     14496    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     14688    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16088    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     17596    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     19076    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1efd76-7b3a-4da2-ac04-99871942ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjinzhaot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b7a7146c3340d495a3e026c715e8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\z910l567\\Desktop\\New folder\\GC\\wandb\\run-20231123_130953-syolcb71</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jinzhaot/t5_encoder_classifier/runs/syolcb71' target=\"_blank\">ethereal-bird-9</a></strong> to <a href='https://wandb.ai/jinzhaot/t5_encoder_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jinzhaot/t5_encoder_classifier' target=\"_blank\">https://wandb.ai/jinzhaot/t5_encoder_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jinzhaot/t5_encoder_classifier/runs/syolcb71' target=\"_blank\">https://wandb.ai/jinzhaot/t5_encoder_classifier/runs/syolcb71</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jinzhaot/t5_encoder_classifier/runs/syolcb71?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x283a4ef5030>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wandb for experiment tracking\n",
    "wandb.init(project=\"t5_encoder_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c87861-11d4-4481-abf8-a900fb52f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "class Config:\n",
    "    model_name = \"T5-small-fce-s_10_epoch_11_15\"\n",
    "    num_labels = 2  # Adjust based on your dataset\n",
    "    batch_size = 512\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 5\n",
    "    max_length = 128  # Adjust as needed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfdafa8-c0eb-4422-abf1-7bc9d1e413a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0 = pd.read_csv('classification_data/train_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7e295d-1353-4734-b157-4da12db47339",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = pd.read_csv('classification_data/c4_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa441a61-605b-46b5-8fc0-f8ac7714c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = pd.read_csv('classification_data/syth_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a57e415-0563-4af0-bc7e-be2caf0407c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df_0, train_df_1, train_df_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3a8641-0f5a-4e76-9af9-8b2b2fff7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9f763d-0105-4c04-9345-a36033b2b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df_0\n",
    "del train_df_1\n",
    "del train_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a02efc9-0a2e-41fa-9a01-5650117fb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('classification_data/test_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737c7e69-7a48-4314-bd1a-5eed02051532",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "557c2745-68ab-4159-8e2f-be06150ef17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for text classification\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset.iloc[idx]\n",
    "        text = data['text']\n",
    "        label = data['label']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6542267-f598-4387-9c8b-59418c7acbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier model\n",
    "class T5Classifier(nn.Module):\n",
    "    def __init__(self, t5_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.t5 = t5_model\n",
    "        hidden_size = t5_model.config.d_model\n",
    "        # Additional layers for a deeper classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.t5(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        first_token_tensor = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(first_token_tensor)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68689461-e442-42e9-a6ab-84d3c3cf1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5Classifier(\n",
       "  (t5): T5EncoderModel(\n",
       "    (shared): Embedding(32128, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(Config.model_name)\n",
    "model = T5EncoderModel.from_pretrained(Config.model_name)\n",
    "classifier_model = T5Classifier(model, Config.num_labels)\n",
    "classifier_model.load_state_dict(torch.load('t5_encoder_classifier_11_21.pth'))\n",
    "classifier_model.to(Config.device)\n",
    "\n",
    "# Freeze the T5 encoder\n",
    "# for param in classifier_model.t5.parameters():\n",
    "#     param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6fc75f8-26f7-4cdc-9f79-eedfdea80045",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(train_df, tokenizer, Config.max_length)\n",
    "val_dataset = TextClassificationDataset(test_df, tokenizer, Config.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2a5dd4-8ceb-4075-8b2f-8f6b634f0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d89b3b82-b3c8-454f-9d70-47f365f1912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z910l567\\AppData\\Local\\miniconda3\\envs\\dl\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(classifier_model.parameters(), lr=Config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54fb7ee4-5e9a-4343-9a66-c56b1348ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    correct = (predicted == labels).float()\n",
    "    return correct.sum() / len(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53cdc230-5cc3-4840-aa6b-39ab3aa5f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.2201, Train Accuracy: 0.9111, Validation Loss: 0.5290, Validation Accuracy: 0.7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 0.1910, Train Accuracy: 0.9239, Validation Loss: 0.5306, Validation Accuracy: 0.7607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 0.1810, Train Accuracy: 0.9282, Validation Loss: 0.5261, Validation Accuracy: 0.7611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 0.1745, Train Accuracy: 0.9310, Validation Loss: 0.5287, Validation Accuracy: 0.7618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.1697, Train Accuracy: 0.9330, Validation Loss: 0.5276, Validation Accuracy: 0.7618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Training and validation loop with tqdm\n",
    "for epoch in range(Config.num_epochs):\n",
    "    classifier_model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs} Training\", leave=False)\n",
    "\n",
    "    for batch in train_progress_bar:\n",
    "        inputs = batch['input_ids'].to(Config.device)\n",
    "        attention_mask = batch['attention_mask'].to(Config.device)\n",
    "        labels = batch['labels'].to(Config.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_model(inputs, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        accuracy = calculate_accuracy(outputs, labels)\n",
    "        total_accuracy += accuracy\n",
    "        train_progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\", 'accuracy': f\"{accuracy.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_train_accuracy = total_accuracy / len(train_loader)\n",
    "    wandb.log({\"Train Loss\": avg_train_loss, \"Train Accuracy\": avg_train_accuracy})\n",
    "\n",
    "    classifier_model.eval()\n",
    "    total_eval_loss = 0\n",
    "    total_eval_accuracy = 0\n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs} Validation\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            inputs = batch['input_ids'].to(Config.device)\n",
    "            attention_mask = batch['attention_mask'].to(Config.device)\n",
    "            labels = batch['labels'].to(Config.device)\n",
    "\n",
    "            outputs = classifier_model(inputs, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "            accuracy = calculate_accuracy(outputs, labels)\n",
    "            total_eval_accuracy += accuracy\n",
    "            val_progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\", 'accuracy': f\"{accuracy.item():.4f}\"})\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "    wandb.log({\"Validation Loss\": avg_val_loss, \"Validation Accuracy\": avg_val_accuracy})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{Config.num_epochs} - Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a38efff9-e43c-4dc0-b18d-04c6ceedd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(classifier_model.state_dict(), \"t5_encoder_classifier_11_23.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75045a3f-963a-4f33-9737-50300623b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(input_text):\n",
    "  batch = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(Config.device)\n",
    "  result = classifier_model(**batch)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ebe86a-fae1-4973-909b-ed115eea458d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0af33e67-a5bd-483f-8527-640475d74dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2851, -1.1959]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"Jeff runs a mile and drops his keys.\"\n",
    "print(classify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fb1592c-b8ee-4c8f-bc61-50f7d16a30fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3362, -1.2404]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"because they spent time on unmeaningful subjects.\"\n",
    "print(classify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bbce821-cbc3-4d3c-94a2-45defc57e8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0737,  1.0736]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"My husband engineer.\"\n",
    "print(classify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b91ff94b-6a53-4e5a-9b2f-082adba8fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7015, -1.5682]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"Although I've known him for a while, I still can't believe how stubborn he is.\"\n",
    "print(classify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25e523-ca2a-47d3-9283-6b49385c1395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
